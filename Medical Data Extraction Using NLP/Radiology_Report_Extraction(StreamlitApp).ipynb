{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEK4bKkkxwa4",
        "outputId": "a99e8b1f-68ef-4cc9-b9bc-c1d0ae04be56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip -q install streamlit \"transformers>=4.43.3\" \"accelerate>=0.33.0\" \\\n",
        "\"bitsandbytes>=0.43.1\" \"peft>=0.11.1\" \"datasets>=2.20.0\" \"evaluate>=0.4.2\" \\\n",
        "\"pandas>=2.2.2\" \"numpy>=1.26.0\" \"tqdm>=4.66.4\" \"sentencepiece>=0.2.0\" spacy\n",
        "\n",
        "# optional for quick tokenization-only stuff\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Streamlit doesn't need Metaflow env here; only HF token if you‚Äôll load gated Llama.\n",
        "if \"HF_TOKEN\" not in os.environ or not os.environ[\"HF_TOKEN\"]:\n",
        "    try:\n",
        "        from getpass import getpass\n",
        "        os.environ[\"HF_TOKEN\"] = getpass(\"Paste your HF token (press Enter to skip for BioGPT): \")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(\"HF_TOKEN set?\" , bool(os.environ.get(\"HF_TOKEN\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4zNko5dyltv",
        "outputId": "24e6c1e3-8a2e-4fb9-fa98-2907075e6b81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste your HF token (press Enter to skip for BioGPT): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "HF_TOKEN set? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile med_helpers.py\n",
        "import re, json\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "\n",
        "FIELDS = [\"Examination\",\"Clinical\",\"Findings\",\"Impression\"]\n",
        "\n",
        "ABBREV_MAP = {\n",
        "    \"wnl\": \"within normal limits\",\n",
        "    \"nl\": \"normal\",\n",
        "}\n",
        "\n",
        "def norm_txt(x: str) -> str:\n",
        "    if not isinstance(x, str):\n",
        "        return \"\"\n",
        "    x = x.strip()\n",
        "    # remove code fences if the model wrapped JSON\n",
        "    x = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", x.strip(), flags=re.I)\n",
        "    x = x.lower()\n",
        "    x = re.sub(r\"[ \\t\\r\\n]+\", \" \", x)\n",
        "    x = re.sub(r\"[,:;]+\", \" \", x)\n",
        "    x = re.sub(r\"\\s{2,}\", \" \", x).strip(\" .\")\n",
        "    for k,v in ABBREV_MAP.items():\n",
        "        x = re.sub(rf\"\\b{k}\\b\", v, x)\n",
        "    return x\n",
        "\n",
        "def build_prompt(report_text: str, fields=FIELDS) -> str:\n",
        "    # keep your intent/format; add explicit ‚Äústart/end with braces‚Äù hint\n",
        "    max_chars = {\"Examination\":120,\"Clinical\":160,\"Findings\":600,\"Impression\":300}\n",
        "    quoted_keys = \", \".join([f'\"{k}\"' for k in fields])\n",
        "\n",
        "    role = (\"You are a precise medical information extraction engine. \"\n",
        "            \"Your only job is to extract specific sections from the given radiology report.\")\n",
        "    schema = (\n",
        "        \"Return a single JSON object with exactly these keys and string values:\\n\"\n",
        "        f\"{quoted_keys}.\\n\"\n",
        "        \"The JSON must be valid and parseable by a standard JSON parser. \"\n",
        "        \"Begin your output with '{' and end with '}'. \"\n",
        "        \"No extra keys, no comments, no explanations, no markdown.\"\n",
        "    )\n",
        "    constraints = (\n",
        "        \"Rules:\\n\"\n",
        "        \"1) Copy or lightly paraphrase from the report; do NOT invent clinical facts.\\n\"\n",
        "        \"2) If a section is missing or truly unspecified, use an empty string \\\"\\\".\\n\"\n",
        "        \"3) Remove patient identifiers/PHI. Keep clinical terminology intact.\\n\"\n",
        "        \"4) Keep each field concise and readable. Do not include section headers in values.\\n\"\n",
        "        \"5) Do not include JSON code fences. Output JSON only.\\n\"\n",
        "        \"6) Respect length limits (characters, not tokens):\\n\"\n",
        "        f\"   - Examination: <= {max_chars['Examination']}\\n\"\n",
        "        f\"   - Clinical:    <= {max_chars['Clinical']}\\n\"\n",
        "        f\"   - Findings:    <= {max_chars['Findings']}\\n\"\n",
        "        f\"   - Impression:  <= {max_chars['Impression']}\\n\"\n",
        "        \"7) If multiple mentions exist for a section, merge succinctly into one value.\\n\"\n",
        "        \"8) Normalize whitespace: single spaces, no trailing punctuation unless natural.\\n\"\n",
        "        \"9) Do not include quotes inside values unless they are part of the clinical text.\"\n",
        "    )\n",
        "    few_shot = (\n",
        "        \"Examples:\\n\"\n",
        "        \"Example 1 (well-structured input):\\n\"\n",
        "        \"Input:\\n\"\n",
        "        \"  Exam: Chest X-ray PA and lateral\\n\"\n",
        "        \"  Clinical indication: Dyspnea and cough for 3 days.\\n\"\n",
        "        \"  Findings: Cardiomediastinal silhouette normal. No focal consolidation or effusion.\\n\"\n",
        "        \"  Impression: No acute cardiopulmonary disease.\\n\"\n",
        "        \"JSON:\\n\"\n",
        "        \"{\\n\"\n",
        "        \"  \\\"Examination\\\": \\\"Chest X-ray PA and lateral\\\",\\n\"\n",
        "        \"  \\\"Clinical\\\": \\\"Dyspnea and cough for 3 days\\\",\\n\"\n",
        "        \"  \\\"Findings\\\": \\\"Cardiomediastinal silhouette normal. No focal consolidation or effusion\\\",\\n\"\n",
        "        \"  \\\"Impression\\\": \\\"No acute cardiopulmonary disease\\\"\\n\"\n",
        "        \"}\\n\"\n",
        "        \"\\n\"\n",
        "        \"Example 2 (messy input, missing Impression):\\n\"\n",
        "        \"Input:\\n\"\n",
        "        \"  ... portable ap chest performed at bedside ...\\n\"\n",
        "        \"  hx: fever; r/o pneumonia. lungs clear; no focal opacity; heart size wnl.\\n\"\n",
        "        \"JSON:\\n\"\n",
        "        \"{\\n\"\n",
        "        \"  \\\"Examination\\\": \\\"Portable AP chest radiograph\\\",\\n\"\n",
        "        \"  \\\"Clinical\\\": \\\"Fever; rule out pneumonia\\\",\\n\"\n",
        "        \"  \\\"Findings\\\": \\\"Lungs clear; no focal opacity; cardiac size within normal limits\\\",\\n\"\n",
        "        \"  \\\"Impression\\\": \\\"\\\"\\n\"\n",
        "        \"}\\n\"\n",
        "    )\n",
        "    task = f\"Input report:\\n{report_text}\\n\\nNow output ONLY the JSON object:\"\n",
        "    return \"\\n\".join([role, \"\", schema, \"\", constraints, \"\", few_shot, task])\n",
        "\n",
        "def strip_code_fences(text: str) -> str:\n",
        "    return re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", text.strip(), flags=re.I)\n",
        "\n",
        "def safe_json(text: str) -> Dict[str,str]:\n",
        "    if not isinstance(text, str):\n",
        "        return {}\n",
        "    text = strip_code_fences(text)\n",
        "\n",
        "    # direct\n",
        "    try:\n",
        "        obj = json.loads(text)\n",
        "        if isinstance(obj, dict): return obj\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # last {...} block\n",
        "    blocks = re.findall(r\"\\{[\\s\\S]*\\}\", text)\n",
        "    for cand in reversed(blocks):\n",
        "        try:\n",
        "            obj = json.loads(cand)\n",
        "            if isinstance(obj, dict): return obj\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    # lenient key:\"value\" pairs (no nested quotes)\n",
        "    out = {}\n",
        "    for k in FIELDS:\n",
        "        mm = re.search(rf'\"{re.escape(k)}\"\\s*:\\s*\"([^\"]*)\"', text)\n",
        "        if mm:\n",
        "            out[k] = mm.group(1)\n",
        "    return out\n",
        "\n",
        "# ----------------- Regex fallback extractor (fixed) -----------------\n",
        "_header_map = {\n",
        "    \"examination\": [\"examination\", \"exam\", \"study\", \"procedure\"],\n",
        "    \"clinical\":    [\"clinical history\", \"history\", \"clinical indication\", \"indication\"],\n",
        "    \"findings\":    [\"findings\", \"results\"],\n",
        "    \"impression\":  [\"impression\", \"conclusion\", \"opinion\", \"assessment\"],\n",
        "}\n",
        "\n",
        "# Build a single pattern capturing any header and its value once.\n",
        "def _compile_pattern():\n",
        "    all_headers = []\n",
        "    for canon, names in _header_map.items():\n",
        "        for n in names:\n",
        "            all_headers.append((canon, n))\n",
        "    # longer names first\n",
        "    all_headers.sort(key=lambda x: -len(x[1]))\n",
        "    header_group = \"|\".join([re.escape(n) for _, n in all_headers])\n",
        "    # Capture 'header' then 'value' until next header or end. Allow optional ':' or '-' after header.\n",
        "    pat = rf\"(?P<header>(?:{header_group}))\\s*[:\\-]?\\s*(?P<value>.*?)(?=(?:{header_group})\\s*[:\\-]?|$)\"\n",
        "    return re.compile(pat, flags=re.I|re.S)\n",
        "\n",
        "_SECTION_RE = _compile_pattern()\n",
        "\n",
        "# Quick lookup from name -> canonical key\n",
        "_name_to_canon = {}\n",
        "for canon, names in _header_map.items():\n",
        "    for n in names:\n",
        "        _name_to_canon[n.lower()] = canon\n",
        "\n",
        "def rule_extract_sections(text: str) -> Dict[str,str]:\n",
        "    if not isinstance(text, str):\n",
        "        return {k:\"\" for k in FIELDS}\n",
        "    d = {\"Examination\":\"\", \"Clinical\":\"\", \"Findings\":\"\", \"Impression\":\"\"}\n",
        "    for m in _SECTION_RE.finditer(text):\n",
        "        header = (m.group(\"header\") or \"\").strip().lower()\n",
        "        value  = (m.group(\"value\")  or \"\").strip()\n",
        "        canon = _name_to_canon.get(header)\n",
        "        if canon:\n",
        "            key = canon.capitalize()\n",
        "            val = re.sub(r\"\\s+\", \" \", value).strip()\n",
        "            d[key] = norm_txt(val)\n",
        "    return d\n",
        "\n",
        "# ---------- Metrics ----------\n",
        "def token_f1(pred: str, gold: str) -> Tuple[float,float,float]:\n",
        "    ptoks = [t for t in re.findall(r\"\\w+\", norm_txt(pred))]\n",
        "    gtoks = [t for t in re.findall(r\"\\w+\", norm_txt(gold))]\n",
        "    if len(ptoks) == 0 and len(gtoks) == 0:\n",
        "        return 1.0, 1.0, 1.0\n",
        "    if len(ptoks) == 0 or len(gtoks) == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    pset, gset = set(ptoks), set(gtoks)\n",
        "    inter = len(pset & gset)\n",
        "    prec = inter / max(1, len(pset))\n",
        "    rec  = inter / max(1, len(gset))\n",
        "    f1   = (0.0 if prec+rec==0 else 2*prec*rec/(prec+rec))\n",
        "    return prec, rec, f1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx4m_BaG0m6S",
        "outputId": "263e8436-54ca-4cf1-9c9f-2e08489d6bc2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting med_helpers.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os, json, re\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import evaluate as hf_eval\n",
        "\n",
        "from med_helpers import (\n",
        "    FIELDS, build_prompt, safe_json, norm_txt, token_f1, rule_extract_sections\n",
        ")\n",
        "\n",
        "st.set_page_config(page_title=\"Radiology IE ‚Äî Prompted LLM\", page_icon=\"ü©∫\", layout=\"wide\")\n",
        "st.title(\"ü©∫ Radiology Information Extraction (Prompt-engineered LLM)\")\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Model & Inference Settings\")\n",
        "    model_choice = st.selectbox(\n",
        "        \"Choose a model\",\n",
        "        [\n",
        "            \"meta-llama/Llama-3.2-1B (gated ‚Äî requires HF token)\",\n",
        "            \"microsoft/BioGPT (ungated ‚Äî smaller)\",\n",
        "        ],\n",
        "        index=0\n",
        "    )\n",
        "    max_new_tokens = st.slider(\"Max new tokens\", 64, 512, 256, 32)\n",
        "    do_sample = st.checkbox(\"Sampling (otherwise greedy)\", value=False)\n",
        "    temperature = st.slider(\"Temperature\", 0.0, 1.5, 0.0, 0.1)\n",
        "    use_regex_fallback = st.checkbox(\"Enable regex fallback when JSON is missing/empty\", value=True)\n",
        "    st.caption(\"Tip: keep greedy decoding (temperature=0) for deterministic JSON.\")\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"Evaluation Settings\")\n",
        "    show_debug = st.checkbox(\"Show raw generations\", value=False)\n",
        "\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
        "chosen_model = \"meta-llama/Llama-3.2-1B\" if \"Llama\" in model_choice else \"microsoft/BioGPT\"\n",
        "\n",
        "@st.cache_resource(show_spinner=True)\n",
        "def load_model(model_name: str, hf_token: str | None):\n",
        "    bnb = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb,\n",
        "        use_auth_token=hf_token\n",
        "    )\n",
        "    model.eval()\n",
        "    model.config.use_cache = True\n",
        "    return tokenizer, model\n",
        "\n",
        "try:\n",
        "    tok, model = load_model(chosen_model, HF_TOKEN)\n",
        "    st.success(f\"Loaded model: {chosen_model}\")\n",
        "except Exception as e:\n",
        "    st.error(f\"Failed to load model `{chosen_model}`. If it's gated (Llama), set HF_TOKEN in Colab. Error: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "def generate_json(report_text: str) -> tuple[dict, str, dict]:\n",
        "    \"\"\"Returns (clean_json, raw_text, fallback_used_details)\"\"\"\n",
        "    prompt = build_prompt(report_text)\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=do_sample,\n",
        "            temperature=temperature,\n",
        "            pad_token_id=tok.pad_token_id,\n",
        "            eos_token_id=tok.eos_token_id,\n",
        "        )\n",
        "    raw = tok.decode(out_ids[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
        "    js = safe_json(raw)\n",
        "\n",
        "    # Build normalized dict of predictions\n",
        "    pred = {k: norm_txt(js.get(k, \"\")) for k in FIELDS}\n",
        "\n",
        "    fallback_info = {\"used\": False, \"filled_from_regex\": []}\n",
        "    if use_regex_fallback:\n",
        "        # If any field is empty, attempt regex extraction and fill missing fields\n",
        "        regex_out = rule_extract_sections(report_text)\n",
        "        for k in FIELDS:\n",
        "            if not pred[k] and regex_out.get(k):\n",
        "                pred[k] = regex_out[k]\n",
        "                fallback_info[\"used\"] = True\n",
        "                fallback_info[\"filled_from_regex\"].append(k)\n",
        "\n",
        "    return pred, raw, fallback_info\n",
        "\n",
        "# ===== Single report tab =====\n",
        "tab1, tab2 = st.tabs([\"üîé Single Report\", \"üìä Batch Evaluation (CSV)\"])\n",
        "\n",
        "with tab1:\n",
        "    st.subheader(\"Single Report Extraction\")\n",
        "    report = st.text_area(\"Paste a radiology report\", height=200, placeholder=\"Enter report text‚Ä¶\")\n",
        "    if st.button(\"Extract\"):\n",
        "        if not report.strip():\n",
        "            st.warning(\"Please paste a report.\")\n",
        "        else:\n",
        "            pred, raw, fb = generate_json(report)\n",
        "            colA, colB = st.columns([1,1])\n",
        "            with colA:\n",
        "                st.write(\"**Predicted JSON**\")\n",
        "                st.code(json.dumps(pred, indent=2), language=\"json\")\n",
        "                if fb[\"used\"]:\n",
        "                    st.info(f\"Regex fallback filled: {', '.join(fb['filled_from_regex'])}\")\n",
        "            with colB:\n",
        "                st.write(\"**Structured Output**\")\n",
        "                for f in FIELDS:\n",
        "                    st.markdown(f\"**{f}:** {pred[f] or '‚Äî'}\")\n",
        "            if show_debug:\n",
        "                with st.expander(\"Raw Generation\"):\n",
        "                    st.code(raw)\n",
        "\n",
        "with tab2:\n",
        "    st.subheader(\"Batch Evaluation\")\n",
        "    st.markdown(\n",
        "        \"Upload a CSV with a `text` column and (optionally) any of the gold columns: \"\n",
        "        \"`Examination`, `Clinical`, `Findings`, `Impression`.\"\n",
        "    )\n",
        "    file = st.file_uploader(\"Choose CSV\", type=[\"csv\"])\n",
        "    if file:\n",
        "        df = pd.read_csv(file)\n",
        "        # Rename so the rest of the code still works without modification\n",
        "        if \"ReportText\" in df.columns and \"text\" not in df.columns:\n",
        "          df.rename(columns={\"ReportText\": \"text\"}, inplace=True)\n",
        "\n",
        "        if \"text\" not in df.columns:\n",
        "            st.error(\"CSV must include a `text` column.\")\n",
        "        else:\n",
        "            df[\"text\"] = df[\"text\"].astype(str)\n",
        "            preds = {f\"pred_{f}\":[] for f in FIELDS}\n",
        "            raws, fb_cols = [], []\n",
        "            prog = st.progress(0)\n",
        "            for i, t in enumerate(df[\"text\"].tolist(), start=1):\n",
        "                pr, raw, fb = generate_json(t)\n",
        "                for f in FIELDS:\n",
        "                    preds[f\"pred_{f}\"].append(pr[f])\n",
        "                raws.append(raw)\n",
        "                fb_cols.append(\",\".join(fb[\"filled_from_regex\"]) if fb[\"used\"] else \"\")\n",
        "                prog.progress(i/len(df))\n",
        "            for k, v in preds.items():\n",
        "                df[k] = v\n",
        "            if show_debug:\n",
        "                df[\"raw\"] = raws\n",
        "            df[\"regex_fallback_filled\"] = fb_cols\n",
        "\n",
        "            rouge = hf_eval.load(\"rouge\")\n",
        "            per_field = {}\n",
        "            for f in FIELDS:\n",
        "                if f in df.columns:\n",
        "                    gold = df[f].fillna(\"\").apply(norm_txt).tolist()\n",
        "                    pred = df[f\"pred_{f}\"].fillna(\"\").apply(norm_txt).tolist()\n",
        "                    exact = np.mean([p==g for p,g in zip(pred,gold)])\n",
        "                    prfs = [token_f1(p,g) for p,g in zip(pred,gold)]\n",
        "                    prec = float(np.mean([x[0] for x in prfs]))\n",
        "                    rec  = float(np.mean([x[1] for x in prfs]))\n",
        "                    f1   = float(np.mean([x[2] for x in prfs]))\n",
        "                    rfs = []\n",
        "                    for p,g in zip(pred,gold):\n",
        "                        sc = rouge.compute(predictions=[p], references=[g])\n",
        "                        rfs.append(sc.get(\"rougeL\", 0.0))\n",
        "                    rougeL = float(np.mean(rfs))\n",
        "                    per_field[f] = {\n",
        "                        \"exact_match\": float(exact),\n",
        "                        \"token_precision\": prec,\n",
        "                        \"token_recall\": rec,\n",
        "                        \"token_f1\": f1,\n",
        "                        \"rougeL_f1\": rougeL\n",
        "                    }\n",
        "                else:\n",
        "                    per_field[f] = \"no_gold\"\n",
        "\n",
        "            scored = [f for f,v in per_field.items() if isinstance(v, dict)]\n",
        "            macro = (\n",
        "                {\n",
        "                    \"exact_match\": float(np.mean([per_field[f][\"exact_match\"] for f in scored])),\n",
        "                    \"token_precision\": float(np.mean([per_field[f][\"token_precision\"] for f in scored])),\n",
        "                    \"token_recall\": float(np.mean([per_field[f][\"token_recall\"] for f in scored])),\n",
        "                    \"token_f1\": float(np.mean([per_field[f][\"token_f1\"] for f in scored])),\n",
        "                    \"rougeL_f1\": float(np.mean([per_field[f][\"rougeL_f1\"] for f in scored])),\n",
        "                }\n",
        "                if scored else \"no_gold\"\n",
        "            )\n",
        "\n",
        "            st.markdown(\"### Metrics\")\n",
        "            st.json({\"per_field\": per_field, \"macro\": macro})\n",
        "\n",
        "            st.markdown(\"### Preview (first 50 rows)\")\n",
        "            st.dataframe(df.head(50))\n",
        "\n",
        "            st.download_button(\n",
        "                \"Download full predictions CSV\",\n",
        "                data=df.to_csv(index=False).encode(\"utf-8\"),\n",
        "                file_name=\"predictions.csv\",\n",
        "                mime=\"text/csv\"\n",
        "            )\n",
        "\n",
        "st.markdown(\"---\")\n",
        "with st.expander(\"Show Prompt Template\"):\n",
        "    st.code(build_prompt(\"...example report...\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL3Kc-3d0r8P",
        "outputId": "acfec5dc-1de5-4241-836d-548e5e74eeea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clean up any old processes ---\n",
        "!pkill -f cloudflared >/dev/null 2>&1 || true\n",
        "\n",
        "# --- 1) Start Streamlit (background) ---\n",
        "!streamlit run app.py --server.port 8501 --server.headless true &>/content/app.log &\n",
        "\n",
        "# --- 2) Wait for Streamlit to come up on localhost:8501 ---\n",
        "import time, urllib.request, urllib.error\n",
        "\n",
        "def wait_for_streamlit(url=\"http://127.0.0.1:8501\", timeout=120):\n",
        "    t0 = time.time()\n",
        "    while time.time() - t0 < timeout:\n",
        "        try:\n",
        "            with urllib.request.urlopen(url, timeout=2) as r:\n",
        "                if r.status == 200:\n",
        "                    return True\n",
        "        except Exception:\n",
        "            pass\n",
        "        time.sleep(1)\n",
        "    return False\n",
        "\n",
        "print(\"‚è≥ Waiting for Streamlit to start...\")\n",
        "ok = wait_for_streamlit()\n",
        "print(\"Streamlit ready:\", ok)\n",
        "if not ok:\n",
        "    print(\"‚ö†Ô∏è Streamlit didn't become ready in time. Dumping last 60 lines of app.log for clues:\")\n",
        "    !tail -n 60 /content/app.log\n",
        "    raise SystemExit\n",
        "\n",
        "# --- 3) Download cloudflared if needed ---\n",
        "import os, subprocess, re, sys\n",
        "\n",
        "BIN = \"/content/cloudflared\"\n",
        "if not os.path.exists(BIN):\n",
        "    !wget -q -O /content/cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "    !chmod +x /content/cloudflared\n",
        "\n",
        "# --- 4) Launch cloudflared and parse its logs for the public URL ---\n",
        "LOG = \"/content/cf.log\"\n",
        "# Start cloudflared in the background, logging to a file\n",
        "p = subprocess.Popen([BIN, \"tunnel\", \"--no-autoupdate\", \"--url\", \"http://127.0.0.1:8501\"],\n",
        "                     stdout=open(LOG, \"w\"), stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "# Tail the log looking for the URL\n",
        "public_url = None\n",
        "pattern = re.compile(r\"(https://[a-z0-9\\-]+\\.trycloudflare\\.com)\", re.I)\n",
        "\n",
        "print(\"‚è≥ Waiting for cloudflared URL...\")\n",
        "for _ in range(240):  # ~4 minutes\n",
        "    time.sleep(1)\n",
        "    try:\n",
        "        with open(LOG, \"r\") as f:\n",
        "            txt = f.read()\n",
        "            m = pattern.search(txt)\n",
        "            if m:\n",
        "                public_url = m.group(1)\n",
        "                break\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "if public_url:\n",
        "    print(\"‚úÖ Your Streamlit app is live at:\", public_url)\n",
        "    print(\"Keep this cell running to keep the tunnel alive.\")\n",
        "else:\n",
        "    print(\"‚ùå Could not detect the tunnel URL.\")\n",
        "    print(\"Last 80 lines of cloudflared log:\")\n",
        "    !tail -n 80 /content/cf.log\n",
        "    print(\"\\nLast 60 lines of app.log:\")\n",
        "    !tail -n 60 /content/app.log\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkFR4kn00r47",
        "outputId": "d0be1dc4-857c-47ce-9b9b-076bd3caa80d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "‚è≥ Waiting for Streamlit to start...\n",
            "Streamlit ready: True\n",
            "‚è≥ Waiting for cloudflared URL...\n",
            "‚úÖ Your Streamlit app is live at: https://notified-lexmark-qui-singles.trycloudflare.com\n",
            "Keep this cell running to keep the tunnel alive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D7Hm4ChY0r2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OtuKl4HV0rz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EWxNy6VL0rxV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}