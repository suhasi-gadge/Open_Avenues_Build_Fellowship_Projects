# -*- coding: utf-8 -*-
"""QWEN0.5B_Week02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1taZ7LjWsW2XK5h_NBjKsAGRMwW_TUs1y
"""

!pip install pandas numpy matplotlib seaborn nltk gensim pyLDAvis wordcloud spacy networkx textstat

# -*- coding: utf-8 -*-
"""week01_eda.py
ED A pipeline for medical radiology reports: business-focused visuals and field-specific analyses.
"""

# ─── Cell 1: Install & Import Libraries ─────────────────────────────────────────────
# Uncomment to install dependencies when needed:
# !pip install pandas numpy matplotlib seaborn wordcloud nltk scikit-learn gensim pyLDAvis networkx

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import nltk
import re
import networkx as nx
from gensim import corpora, models
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
from itertools import combinations
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from collections import Counter

# Ensure NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt_tab')
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

sns.set_style("whitegrid")

# ─── Cell 2: Load & Inspect Data ──────────────────────────────────────────────────
df = pd.read_csv("open_ave_data.csv")

# Drop stray index column
if "Unnamed: 0" in df.columns:
    df.drop(columns=["Unnamed: 0"], inplace=True)

print(f"Total records: {len(df)}")
print("Fields:", df.columns.tolist())
display(df.head(3))

import pandas as pd
import re
import string
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stopwords



# --- 2. Text‐cleaning helpers ---
def clean_text(text):
    if pd.isnull(text):
        return ""
    text = text.lower()
    text = re.sub(rf"[{re.escape(string.punctuation)}]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    tokens = [tok for tok in text.split() if tok not in stopwords]
    return " ".join(tokens)

synonym_pattern = re.compile(
    r"^(clinical history|history|indication)\s*[:\-]?\s*", flags=re.IGNORECASE
)
def clean_clinicaldata(text):
    if pd.isnull(text):
        return ""
    txt = text.strip()
    txt = synonym_pattern.sub("clinical history ", txt)
    return clean_text(txt)

# --- 3. Extract date/time regex & columns ---
# adjust to match your date/time formats
date_time_re = (
    r"(?P<dt>(?:\d{1,2}[/-]\d{1,2}[/-]\d{2,4}|\d{4}-\d{1,2}-\d{1,2})"
    r"(?:\s+\d{1,2}:\d{2}(?:\s?[APMapm]{2})?)?)"
)

# pull from exam_name
ex_dt = df["ExamName"].str.extract(date_time_re)
df["exam_datetime"] = pd.to_datetime(ex_dt["dt"], errors="coerce")

# pull from impressions if missing
imp_dt = df["impression"].str.extract(date_time_re)
df["imp_datetime"] = pd.to_datetime(imp_dt["dt"], errors="coerce")

# fill exam_datetime where missing
mask = df["exam_datetime"].isna() & df["imp_datetime"].notna()
df.loc[mask, "exam_datetime"] = df.loc[mask, "imp_datetime"]

# strip the date/time out of the text fields
df["ExamName"]    = df["ExamName"].str.replace(date_time_re, "", regex=True).str.strip()
df["impression"]  = df["impression"].str.replace(date_time_re, "", regex=True).str.strip()

# --- 4. Clean every text column into clean_<col> ---
text_cols = df.select_dtypes(include=["object"]).columns.tolist()
for col in text_cols:
    cleaner = clean_clinicaldata if col.lower()=="clinicaldata" else clean_text
    df[f"clean_{col}"] = df[col].apply(cleaner)

# --- 4a. Now strip out those specific keywords from the clean_* columns ---
# remove “findings” from clean_findings
df['clean_findings'] = (
    df['clean_findings']
    .str.replace(r'\bfindings\b', '', case=False, regex=True)
    .str.strip()
)

# remove “clinical history”, “clinical data”, “clinical information” from clean_clinicaldata
df['clean_clinicaldata'] = (
    df['clean_clinicaldata']
    .str.replace(
        r'\bclinical history\b|\bclinical data\b|\bclinical information\b',
        '',
        case=False,
        regex=True
    )
    .str.strip()
)

# remove “exam” from clean_ExamName
df['clean_ExamName'] = (
    df['clean_ExamName']
    .str.replace(r'\bexam\b', '', case=False, regex=True)
    .str.strip()
)

# remove “impression” from clean_impression
df['clean_impression'] = (
    df['clean_impression']
    .str.replace(r'\bimpression\b', '', case=False, regex=True)
    .str.strip()
)


# --- 5. Build final clean‐only DataFrame ---
# keep: all clean_<col> plus exam_datetime
clean_cols = [c for c in df.columns if c.startswith("clean_")] + ["exam_datetime"]
clean_df = df[clean_cols]

# --- 6. Save and preview ---
out_csv = "fully_cleaned_data.csv"
clean_df.to_csv(out_csv, index=False)
print(f"Clean data saved to '{out_csv}'\n")
print(clean_df.head())

# ─── Cell 4: Top Clinical Reasons ─────────────────────────────────────────────────
# ─── Normalize ClinicalReason ──────────────────────────────────────────────
import string
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 0) Load only the cleaned data + datetime
clean_df = pd.read_csv("fully_cleaned_data.csv", parse_dates=["exam_datetime"])

# 1) Rename the cleaned clinical-data column for readability
clean_df = clean_df.rename(columns={"clean_clinicaldata": "ClinicalReason"})

# 2) Make a display version (title-case)
clean_df["ClinicalReasonDisplay"] = clean_df["ClinicalReason"].str.title()

# 3) Compute the top 8 reasons
top_reasons = clean_df["ClinicalReasonDisplay"].value_counts().nlargest(8)

# 4) Plot
plt.figure(figsize=(8,4))
sns.barplot(x=top_reasons.values, y=top_reasons.index, palette="Oranges_r")
plt.title("Top Clinical Reasons for Radiology Referral")
plt.xlabel("Number of Exams")
plt.ylabel("Reason")
plt.tight_layout()
plt.show()

"""# **PROMPT**"""

!pip install transformers accelerate evaluate --quiet

import pandas as pd
import json
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from evaluate import load as load_metric

# you said clean_df is already in memory
print("Total reports:", len(clean_df))
clean_df.head(2)



"""QWEN

"""

import re
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# 1) Load Qwen2.5-7B-Instruct
model_name = "Qwen/Qwen2.5-0.5B-Instruct"
tokenizer  = AutoTokenizer.from_pretrained(model_name)
model      = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
model.eval()
device = next(model.parameters()).device

# 2) Build a 5-shot prefix
examples = clean_df.sample(5, random_state=2).reset_index(drop=True)

prefix_lines = [
    "You are a JSON generator for radiology reports.",
    "Return exactly one JSON object with keys: ExamDate, ExamType, Findings, Impression.",
    "Do NOT output any extra text—only the JSON.",
    ""
]
for i, row in examples.iterrows():
    filled = {
        "ExamDate":   str(row["exam_datetime"]),
        "ExamType":   row["clean_ExamName"],
        "Findings":   row["clean_findings"],
        "Impression": row["clean_impression"]
    }
    prefix_lines += [
        f"Example {i+1}:",
        "Report:",
        row["clean_ReportText"],
        "Output JSON:",
        json.dumps(filled, ensure_ascii=False),
        ""
    ]

few_shot_prefix = "\n".join(prefix_lines)

# 3) Parser function
def parse_with_qwen(report_text: str) -> dict:
    prompt = (
        few_shot_prefix +
        "\nNow parse this new report:\n"
        "Report:\n" + report_text + "\n"
        "Output JSON:\n"
    )
    # tokenize + truncate to 512 tokens
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(device)
    # generate with beam search
    out_ids = model.generate(
        **inputs,
        max_new_tokens=256,
        num_beams=5,
        early_stopping=True
    )
    # decode only the newly generated portion
    gen = tokenizer.decode(out_ids[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)
    print(">>> RAW MODEL OUTPUT:\n", gen, "\n")
    # regex‐extract the JSON
    m = re.search(r"\{.*\}", gen, re.DOTALL)
    if not m:
        return {}
    try:
        return json.loads(m.group(0))
    except json.JSONDecodeError:
        return {}

# 4) Test on 5 new samples
for rpt in clean_df["clean_ReportText"].sample(5, random_state=42).tolist():
    result = parse_with_qwen(rpt)
    print("Parsed ➜", result)
    print()



!pip install evaluate --quiet

# ─── 1) Imports & prepare test set ──────────────────────────────────────
import json
from evaluate import load as load_metric

# choose 50 held-out rows (make sure none overlap with your few-shot examples)
test_df = clean_df.drop(examples.index).sample(50, random_state=0).reset_index(drop=True)

# 0) Verify GPU is enabled
import torch
print("CUDA available:", torch.cuda.is_available())

# 1) Prepare a small sample (5 reports)
#    If you already have test_df defined, simply:
sample_reports = (
    test_df["clean_ReportText"]
    .drop(examples.index, errors="ignore")   # remove any few-shot examples
    .sample(5, random_state=0)
    .tolist()
)

# 2) Run parse_with_qwen in parallel
from concurrent.futures import ThreadPoolExecutor
from tqdm.notebook import tqdm

def parse_one(rpt: str):
    return parse_with_qwen(rpt)

with ThreadPoolExecutor(max_workers=4) as executor:
    preds = list(tqdm(
        executor.map(parse_one, sample_reports),
        total=len(sample_reports),
        desc="QWEN parsing (5)"
    ))

# 3) Inspect your 5 predictions
for i, p in enumerate(preds, 1):
    print(f"\n--- Prediction {i} ---\n", json.dumps(p, indent=2))

# --- 4) Compute metrics on the 5 predictions, including Precision & Recall ---

from evaluate import load as load_metric

# 4.1) Load the exact‐match (accuracy) metric
exact = load_metric("accuracy")

# 4.2) Re-sample the same 5 gold records
sample_gold = (
    test_df
    .drop(examples.index, errors="ignore")
    .sample(5, random_state=0)
    .reset_index(drop=True)
)
gold_records_sample = sample_gold[
    ["exam_datetime","clean_ExamName","clean_findings","clean_impression"]
].to_dict(orient="records")

# 4.3) Define your fields and matching gold-keys
fields    = ["ExamDate",   "ExamType",          "Findings",           "Impression"]
gold_keys = ["exam_datetime","clean_ExamName","clean_findings","clean_impression"]

# 4.4) Prepare containers
results = {
    f: {"exact": [], "precision": [], "recall": [], "f1": []}
    for f in fields
}

# 4.5) Helper to compute token‐level P/R/F1
def tok_prf(pred_tokens, ref_tokens):
    if not pred_tokens or not ref_tokens:
        # no tokens → exact only
        return (int(pred_tokens==ref_tokens), 1.0 if pred_tokens==ref_tokens else 0.0, 1.0 if pred_tokens==ref_tokens else 0.0)
    common = sum(min(pred_tokens.count(t), ref_tokens.count(t))
                 for t in set(pred_tokens)|set(ref_tokens))
    p = common / len(pred_tokens)
    r = common / len(ref_tokens)
    f = 2*p*r/(p+r) if (p+r)>0 else 0.0
    return p, r, f

# 4.6) Loop through preds & gold, fill metrics
for pred, gold in zip(preds, gold_records_sample):
    for field, gkey in zip(fields, gold_keys):
        p_val = str(pred.get(field,""))
        g_val = str(gold[gkey])
        # exact‐match
        is_exact = int(p_val == g_val)
        results[field]["exact"].append(is_exact)
        # token‐level P/R/F1
        p_tokens = p_val.split()
        g_tokens = g_val.split()
        p, r, f = tok_prf(p_tokens, g_tokens)
        results[field]["precision"].append(p)
        results[field]["recall"].append(r)
        results[field]["f1"].append(f)

# 4.7) Print a nice summary
print("Field       | Exact% | Token-P% | Token-R% | Token-F1%")
print("------------|--------|----------|----------|-----------")
for f in fields:
    n = len(results[f]["exact"])
    em = 100 * sum(results[f]["exact"])      / n
    pp = 100 * sum(results[f]["precision"])  / n
    rr = 100 * sum(results[f]["recall"])     / n
    ff = 100 * sum(results[f]["f1"])         / n
    print(f"{f:12} | {em:6.2f} | {pp:8.2f} | {rr:8.2f} | {ff:9.2f}")

# Cell 14: Imports & prepare test set (bump from 50 → 200 examples)

import pandas as pd

# assume clean_df and examples are already defined above
test_df = (
    clean_df
    .drop(examples.index)         # remove any few‐shot examples
    .sample(200, random_state=0)  # ↑ bump sample size here (or remove .sample() to use all)
    .reset_index(drop=True)
)

print(f"Preparing to parse {len(test_df)} reports")

# Cell 15: GPU check + parallel parse of all 200 reports

import torch
from concurrent.futures import ThreadPoolExecutor
from tqdm.notebook import tqdm

# 1) Verify GPU
print("CUDA available:", torch.cuda.is_available())

# 2) Grab every report in test_df
sample_reports = test_df["clean_ReportText"].tolist()

# 3) Define worker
def parse_one(rpt: str):
    return parse_with_qwen(rpt)

# 4) Fire off in parallel (4 workers)
with ThreadPoolExecutor(max_workers=4) as executor:
    preds = list(tqdm(
        executor.map(parse_one, sample_reports),
        total=len(sample_reports),
        desc=f"QWEN parsing ({len(sample_reports)})"
    ))

print("Done parsing.")

# Cell 16: Compute metrics on all parsed records (no more re-sampling)

from evaluate import load as load_metric

# 1) Load exact‐match metric
exact = load_metric("accuracy")

# 2) Pull gold annotations directly from test_df
gold_records = test_df[
    ["exam_datetime", "clean_ExamName", "clean_findings", "clean_impression"]
].to_dict(orient="records")

# 3) Define your fields
fields    = ["ExamDate", "ExamType", "Findings", "Impression"]
gold_keys = ["exam_datetime", "clean_ExamName", "clean_findings", "clean_impression"]

# 4) Prepare accumulators
results = {f: {"exact": [], "precision": [], "recall": [], "f1": []} for f in fields}

# 5) Token-level P/R/F1 helper
def tok_prf(pred_tokens, ref_tokens):
    if not pred_tokens or not ref_tokens:
        # treat as exact‐match
        is_exact = pred_tokens == ref_tokens
        return float(is_exact), float(is_exact), float(is_exact)
    common = sum(min(pred_tokens.count(t), ref_tokens.count(t))
                 for t in set(pred_tokens) | set(ref_tokens))
    p = common / len(pred_tokens)
    r = common / len(ref_tokens)
    f = 2*p*r/(p+r) if (p+r)>0 else 0.0
    return p, r, f

# 6) Compute
for pred, gold in zip(preds, gold_records):
    for field, gkey in zip(fields, gold_keys):
        p_val = str(pred.get(field, ""))
        g_val = str(gold[gkey])

        # exact‐match
        results[field]["exact"].append(int(p_val == g_val))

        # token‐P/R/F1
        p_tokens = p_val.split()
        g_tokens = g_val.split()
        p, r, f = tok_prf(p_tokens, g_tokens)
        results[field]["precision"].append(p)
        results[field]["recall"].append(r)
        results[field]["f1"].append(f)

# 7) Print summary
print("Field       | Exact% | Token-P% | Token-R% | Token-F1%")
print("------------|--------|----------|----------|-----------")
for f in fields:
    n  = len(results[f]["exact"])
    em = 100 * sum(results[f]["exact"])      / n
    pp = 100 * sum(results[f]["precision"])  / n
    rr = 100 * sum(results[f]["recall"])     / n
    ff = 100 * sum(results[f]["f1"])         / n
    print(f"{f:12} | {em:6.2f} | {pp:8.2f} | {rr:8.2f} | {ff:9.2f}")





